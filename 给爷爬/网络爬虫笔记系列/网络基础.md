## HTTP 基本原理
### URI 和 URL 
* URI
    * 全称为Uniform Resource Identifier也就是统一资源标识符
    
* URL
    * 全称为Uniform Resource Locator也就是统一资源定位符
    

*举个例子：一本书，它有自己的编码，然后其也对应着的有自己的资源地址这里的资源地址就是我们说的URL，而
这个名字就是我们所说的URN，Uniform Resource Name其中所有的都可以是URI，都可以唯一标识*

### 超文本(Hypertext)
* 网页的源码就可以叫做超文本，也就是HTML文本
### HTTP和HTTPS
* HTTP
    * 一种URL的协议类型，同样的还有ftp，sftp，smb，这些都是协议类型
    * 全称是Hyper Text Transfer Protocol 也叫做超文本传输协议，用于从网络传输超文本数据到本地浏览器的传送协议
    
* HTTPS
    * 全称是Hyper Text Transfer Protocol over Secure Socket Layer也就是安全版本的HTTP也就是HTTP下加入了SSL层，所以简称HTTPS
    * 作用
        * 建立一个信息安全通道来保证数据传输的安全
        * 确认一个网站的真实性，凡是使用了HTTPS的网站都可以通过点击浏览器地址的锁头标志来确认认证信息，也可以通过CA机构颁发的证书来查询
    

*有些时候，当CA证书的颁发地址不是一个被认可的机构时候，我们需要设置忽略整证书的选项，否则SSL连接错误*
### HTTP 请求过程
> 在浏览器输入网址，然后回车，浏览器便向服务器发送了一个请求，网站服务器接收到了这个请求之后，返回了些超文本的数据，浏览器接收到了
> 之后就进行解析操作，把网页的源代码等内容通过解析器解析出来，最后便呈现了这个网络的整体样式

![image.png](https://i.loli.net/2021/01/18/YIieRqwXhFAEDSK.png)

* Size 如果是从缓存中获取的话就会显示from cache
* Time 发送请求到相应的总时间，可以指道我们设置爬取的间隔
> 点击一个条目后可以获得更加详细的信息
* General部分

|名称|内容|
|:---:|:---:|
|Request URL|请求的URL|
|Request Method|请求的方法|
|Status Code|响应状态码|
|Remote Address|远程服务器地址和端口|
|Referrer Policy|为Referrer判断策略|

* Response Headers （响应头）
  > 浏览器标识、 ookies Host 等信息，这是请求的一部分，服务器
会根据请求头内的信息判断请求是否合法，进而作出对应的响应
* Request Headers （请求头）
  > 了服务器的类型、文档类型、日期等信息，浏览器接受到响应后，会解析
响应内容，进而呈现相应的网页内容
  > 


### 请求
> 请求，指的是由客户端向服务端发出，可以分为四个部分：    
> 请求方法（Request Method）  
> 请求的网址（Request URL）    
> 请求头（Request Headers）  
> 请求体（Request Body） 

1. 请求方法     
常见的请求方法有两种： GET和POST        
    GET
> 比如说在浏览器中直接输入URL访问这就是一个GET请求，请求的参数
> 会直接包含在URL里面
https://www.baidu.com/s?wd=python       
> 这就是一个请求，参数wd表示要搜索的关键字，我们这里输入的是python    

   Post
> POST 请求大多在表单提交
时发起 比如，对于 个登录表单，输入用户名和密码后，点击“登录”按钮，这通常会发起一个 POST
请求，其数据通常以表单的形式传输，而不会体现在 URL中

区别：
* GET请求中的参数都包含在URL里面所以数据就可以直接在URL里面看到，而POST请求的URL不会包含这些数据
数据都是以表单的形式发送的，会包含在请求体中
* GET请求提交的数据中最多只有1024字节，而POST方式则没有限制

*登录时候的数据都一般是以POST方式发送的，因为其中包含的是账号密码的一些敏感信息，一般上传文件也都是POST方法*
![image.png](https://i.loli.net/2021/01/18/NFfZP2jadkVLT6Y.png)

2. 请求的网址
请求的网址，即统一资源定位符URL，它可以唯一确定我们想要请求的资源
   
3. 请求头
请求头，用来说明服务器要使用的附加信息，比较重要的信息有Cookie,Referer,User-Agent等等
   

![image.png](https://i.loli.net/2021/01/18/dUBFkO2fe1HtwlK.png)

4. 请求体
请求体一般承载的内容是POST表单中的表单数据，而对于GET请求，请求体则是为空的
   

![image.png](https://i.loli.net/2021/01/18/hI7edxpJjMcZP43.png)

### 响应
1. 响应状态码
200 成功访问， 404 页面未找到， 500服务器内部发生问题
   
2. 响应头
* 响应头包含了服务器对请求的应答信息，包括Content-Type, Server, Set-Cookie等等

![image.png](https://i.loli.net/2021/01/18/wqcr5EdpQ8JxKgb.png)

[HTTP缓存补充阅读资料](https://cloud.tencent.com/developer/article/1359915)

3. 响应体

响应体就是服务器返回回来的网页源代码，以及一些JSON数据或者一些二进制的文件

## 网页基础
### 网页的组成
> 网页可以分为三大部分---HTML，JavaScript,CSS好比说HTML是一个人的骨架，JavaScript相当于肌肉，而CSS是相当于皮肤
1. HTML
* HTML是一种描述网页的语言，其全称是Hyper Text Markup Language 即超文本标记语言，网页包括
文字，按钮，图片，视频，等等各种复杂的元素，其基础架构就是HTML，不同类型的文本通过不同的标签来表示
  比如说图片就使用img，视频使用video标签，段落使用p标签表示，其中布局又通过div嵌套标签所组合而成，这样就
  形成了网页的基本框架
  
2. CSS
> HTML定义了网页的结构，但是只有HTML的网页并不美观，可能只是简单的节点排列，为了让网页更加美观，我们需要借助CSS

*CSS全称叫做Cascading Style Sheets即层叠样式表，“层叠”是指在HTML中引用了数
个样式文件，并且样式发生冲突的时候浏览器仍然能够一句层叠顺序处理，样式指的句时网页中
的文字大小颜色，元素间距排列，CSS是目前唯一的标准*

![image.png](https://i.loli.net/2021/01/18/x2tweoEzcIO1Wym.png)

3. JavaScript
> JavaScript ，简称 JS 是一种脚本语 HTML css 配合使用， 提供给用户的只是 种静态信
息，缺乏交互性 我们在网页里可能会看到 些交互和动画效果，如下载进度条、提示框 轮播图
这通常就是 JavaScript 的功劳 它的出现使得用户与信息之间不只是 种浏览与显示的关系，而是
现了 种实时、动态、交互的页面功能

引入：`<script src: "j query-2 .1. o. j s" ><I script>`

### 网页的结构

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>This is just a Demo</title>
</head>
<body>
<div id="container">
    <div>
        <h2 class="card-title">Hello World</h2>>
        <p>Excelly this is just a simple word</p>
    </div>
</div>>
</body>
</html>
```

### 节点数及节点之间的关系

* 在HTML中，所有标签定义的内容都是一个节点，它们共同构成了一个HTML DOM树

> DOM是W3C（万维网联盟）的标准，其英文全称是Document, Object, Model,即文档对象模型，它定义了访问HTML与XML文档的标准。

![image.png](https://i.loli.net/2021/01/18/3qndgV1CHDGBiQK.png)

W3C DOM标准又分为3个不同的部分

* 核心DOM：针对任何结构化文档的标准模型
* XML DOM：针对任何XML文档的标准类型
* HTML DOM：针对HTML文档格式的标准模型

根据W3C的HTML DOM标准，HTML文档中所有内容都是一个节点

* 整个文档都是一个文档节点
* 每个HTML元素都是元素节点
* HTML元素内的文本都是文本节点
* 每个HTML属性都是属性节点
* 注释是注释节点

![image.png](https://i.loli.net/2021/01/19/2ch5kJSa9oLeDnU.png)

> 通过HTML DOM，树中所有的节点都可以通过JavaScript访问,所有的HTML节点元素均可以被修改，创建，删除
>
> 节点树之间我们常常使用父（parent）子（child）兄弟（sibling）来描述这些节点之间的关系
>
> 父节点拥有子节点，同级的子节点被称为兄弟节点

> 除了根节点之外所有的节点都能拥有任意数量的子节点

![image.png](https://i.loli.net/2021/01/19/eQdugFTP4q1yUCc.png)

### 选择器

![image.png](https://i.loli.net/2021/01/19/8mRK4BiAPT2VX3N.png)

![image.png](https://i.loli.net/2021/01/19/cIBLJbzefaGRMks.png)

## 爬虫的基本原理

### 爬虫概述



1. 获取网页
   * 获取网页的源代码
   * 向网站的服务器发送一个请求，返回回来的就是响应体也就是网页源代码
2. 提取信息
   * 获取之后就是提取我们想要的数据，最常用的方法是正则表达式，但是在构造的时候容易出错
   * 一般的还有CSS选择器，和XPath来提取网页信息的库，比如说Beautiful Soup，pyquery， lxml、等等
3. 保存数据
   * 简单的可以保存为TXT，JSON
   * 或者保存到数据库，如MySQL，MongoDB
   * 保存到远程的服务器，如借助SFTP操作
4. 自动化程序
   * 爬虫就是代替我们来成这份爬取工作的自动程序，它可以在抓取过程中进行各 异常处理、错误重试等操作 ，确保爬取持续高效地运行

### 能抓取怎样的数据

* 网页源代码
* JSON字符串
* 二进制文件：图片和视频和影频。
* 另外其他的一些文件都可以爬取下来，只要是能够访问到的，都可以爬取
* 由于这些都是基于HTTP，HTTPS协议的，所以都是可爬的

### JavaScrip 渲染页面

> 有时候在抓取网页的时候，发现拿到的源代码与我们所设想的不太一样

```html
< ! OOCTYPE html> 
<html lang="en"> 
<head> 
eta charset="UTF-8” >
<title>This is a Oemo</title> 
</head> 
<body> 
<div id=”container” >
</div> 
</body> 
<script src=”app.js”></script> 
</html>
```

这里的body节点里面只有一个id为container的节点，但是注意后面引入了app.js，它负责网站的渲染

所以在直接访问这样的网站的时候，就会由于无法得到JavaScript文件而导致文件的效果不相同，后面将可以使用Selenium，和Splash来实现模拟JavaScript渲染，也可以分析后台的Ajax接口



## 会话与Cookies

### 静态网页与动态网页

* 静态网页
  * 把所有的东西都提前编辑好指定好，然后呈现出来，这种页面就叫做静态页面
  * 优点
    * 加载速度快
    * 编写简单
  * 缺点
    * 可维护性差
* 动态网页
  * 可以动态解析网页的URL的参数的变化，关联数据库，并且动态的呈现不同的多变的内容。

### 无状态HTTP

* 简单一句话：服务端无法记住客户端到底是个啥情况

1. 会话
   * -简单来说就是web会创建一个会话对象，在网页跳转的时候，存储在会话对象中的变量不会消失，而是在整个用户会话中一直存储下去，知道会话过期或者被放弃，服务器就会终止这个对话
2. Cookies
   * 指某些网站为了辨别用户身份 进行会话跟踪而存储在用户本地终端上的数据

* 会话维持

  * 简单来说：当客户端第一次请求服务器的时候，服务器会返回一个请求头中带有Set-Cookie字段的响应给客户端，用来标记是哪一个用户，客户端则会把Cookie保存下来，下一次请求的时候，客户端把Cookie放在与请求头一起的位置，然后发送给服务器，而Cookie中携带了会话的ID信息，服务器通过检查Cookie就可以拿到会话，检查会话来辨别用户的登录信息

  * 如果会话过期，或者说Cookie无效都会引起访问的错误



![image.png](https://i.loli.net/2021/01/19/aPSK1otBMbQIT6q.png)



* Name：  该Cookie的名称，一旦创建，不可修改

* Value： 该Cookie的值，如果值为Unicode字符，则需要字符编码，如果为二进制数据，则需要使用BASE64编码
* Domain： 可以访问该Cookie的域名，例如，如果设置为.zhihu.com,则所有以zhihu.com结尾的域名都可以访问该Cookie
* Max Age： 该Cookie失效的时间，单位为秒，于Expires写在一起，如果说max age 是正数，则表示这个Cookie会在多少秒之后过期，如果为负数则关闭就失效了
* Path： 该Cookie使用的路径，如果设置为/path/则只有这个路径可以访问，如果设置为/则这个域名下所有的页面都可以访问该Cookie
* Size： 此Cookie的大小
* HTTP字段： Cookie的httponly属性，若此属性为true，则只有在HTTP头中带有此Cookie信息，而不能通过document.cookie来访问此Cookie
* Secure： 该Cookie是否仅被使用安全协议传输，安全协议有HTTPS，和SSL等，在网络上传输数据前都是将数据加密，默认为false

> 会话Cookie和持久Cookie

从表面意思来说，会话 Cookie 就是把 Cookie 放在<font color = red>浏览器内存</font>里，浏览器在关闭之后该 Cookie

失效 持久 Cookie 会保存到<font color = red>客户端的硬盘</font>中，下次还可以继续使用，用于长久保持用户登录状态

其实严格来说，没有会话 Cookie 和持久 Cookie 分，只是由 ookie Max Age Expires 字段

决定了过期的时间

因此， 一些持久化登录的网站其实就是把 Cookie 的有效时间和会话有效期设置得比较长，下次夜

再访 页面时 然携带之前的 Cookie ，就可以直接保持登录状态

##  代理的基本原理

* 有些时候我们做爬虫的时候，会发现刚开始的时候爬取的还蛮好的，后面突然就出现了403Forbidden，这时候如果打开网页就会显示你访问的频率太高了。其实这就是网页的一种反爬虫机制，，服务器会检测某个IP在单位时间里面的访问的频次，超过某个阈值之后就会把这个IPBan掉，返回些错误信息，这就是我们所谓的封IP

### 基本原理

* 代理实际上就是我们的代理服务器，英文叫做proxy server ，代理网络用户去获取网络信息，当请求一个网站时候，事发送给了请求到web服务器，web服务器把响应返还给我们，代理服务器就是在本机与服务器之间架起了一个中转站，由代理服务器发起请求，然后再发送给服务器，代理服务器把获得的信息返还给本机，由于代理服务器的IP并不是我们的IP所以这就实现了IP伪装

### 代理的作用

1. 突破自身IP访问限制，访问一些平时不能访问的站点
2. 访问一些单位或者团体的内部资源： 比如使用教育网内地址段免费代理服务器，就可以用于对教育网开放的各类FTP下载上传，以及各类资源查询共享服务
3. 提高访问速度： 通常代理服务器都设置一个较大的硬盘缓冲区，当有外界信息通过的时候，同时也可以将其保存再缓冲区之中，当其他用户访问相同的内容的时候就可以直接俄由缓冲区取出信息传给用户，用来提高访问速度
4. 隐藏真实的IP

### 爬虫代理

爬取过程中不断的替换代理则可以避免IP被封禁的可能，提高爬取效率

### 代理的划分

1. 根据协议划分
   1. FTP代理服务器： 用于访问FTP服务器，一般有上传下载以及缓存的功能，端口一般为21、2121等
   2. HTTP代理服务器： 一般用于访问网站，一般有内容过滤，缓存的功能，端口一般为80、8080、3128
   3. SSL/TLS代理： 访问加密网站、一般有SSL或者TLS加密功能（最高支持128位加密强度）端口位443
   4. RTSP代理： 主要用于访问Real流媒体服务器，一般有缓存的功能，端口一般为554
   5. Telnet代理： 主要用于telnet远程控制（黑客入侵计算机时常用于伪装身份）端口一般为23
   6. POP3/SMTP： 用于POP3/SMTP方式接收邮件，一般有缓存的功能，端口为110、25
   7. SOCKS： 单纯的传递数据包，不关心具体协议的用法，所以速度快很多，一般有缓存的功能端口为1080，SOCKS代理协议又分为SOCKS4、SOCKS5前者支持TCP后者支持TCP和UDP，还支持各种省份验证机制与服务器域名解析。
2. 根据匿名程度区分
   1. 高匿名代理： 将数据包原封不动的转发，在服务段开来就好像一个真的客户端在访问
   2. 普通匿名代理： 会在数据包上做一些改动，服务端有可能认出来这是个代理服务器，也有一定的几率助查到真实的IP，代理服务器常加入的HTTP头有HTTP_VIA和HTTP_X_FORWARDED_FOR
   3. 透明代理： 不但改动数据包，还告诉服务端客户端的真实IP，这种代理可以提高浏览速度，使用内容过滤来提高安全度以外没别的作用，常用的就是内网的硬件防火墙
   4. 间谍代理： 值组织或者个人创建用于记录个人数据传输的数据，然后进行研究监控等目的的代理服务器

### 常见的代理设置

1. 使用网上的免费代理
2. 使用付费代理服务
3. ADSL拨号



