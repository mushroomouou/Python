# pythonçˆ¬å–ç½‘ç»œæ¼«ç”»

# è¦æ±‚
# 1. å·²çŸ¥æ¼«ç”»åå°±å¯ä»¥çˆ¬å–æŒ‡å®šé›†æ•°çš„æ¼«ç”»å¹¶ä¸”å­˜å‚¨èµ·æ¥
# 2. æœ‰æ˜æ˜¾çš„é”™è¯¯å¤„ç†æ“ä½œ
# ç½‘ç«™   http://www.mangabz.com

# å¯¼åŒ…

import requests
from urllib.parse import urlencode
from pyquery import PyQuery
from selenium import webdriver
from pyquery import PyQuery as pq
from pandas import DataFrame
from pandas import Series
import time
import sys
import os
import re

headers = {
    'Referer': 'http://www.mangabz.com/',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36 Edg/88.0.705.63',
    # 'Cookie':'__cfduid=d870b1530435446c9569b766bccc4ff341612766070; MANGABZ_MACHINEKEY=9eb5b82c-c4ce-4b4d-b98c-ac20e7e4b92a; mangabz_newsearch=%5b%7b%22Title%22%3a%22%e4%b8%80%e6%8b%b3%e8%b6%85%e4%ba%ba%22%2c%22Url%22%3a%22%5c%2fsearch%3ftitle%3d%25E4%25B8%2580%25E6%258B%25B3%25E8%25B6%2585%25E4%25BA%25BA%22%7d%5d; UM_distinctid=1778059aed12c8-04ec26c5e466e98-4c3f217f-1fa400-1778059aed2ab; CNZZDATA1278095929=1314768920-1612762169-http%253A%252F%252Fwww.mangabz.com%252F%7C1612762169; CNZZDATA1278095942=1019928459-16127641â€¦278515277=359794671-1612764736-http%253A%252F%252Fwww.mangabz.com%252F%7C1612764736; _Mangabz_Mangamangabz=AAEAAAD/////AQAAAAAAAAAMAgAAAEJJTGlrZS5Ub29scywgVmVyc2lvbj0xLjAuMC4wLCBDdWx0dXJlPW5ldXRyYWwsIFB1YmxpY0tleVRva2VuPW51bGwFAQAAABlJTGlrZS5Ub29scy5Vc2VyUHJpbmNpcGFsBgAAAAlfaWRlbnRpdHkHX3VzZXJJRAlfdXNlck5hbWUIX3JvbGVJRHMIX2lzQWRtaW4dTWFyc2hhbEJ5UmVmT2JqZWN0K19faWRlbnRpdHkDAAEHAAIjU3lzdGVtLlNlY3VyaXR5LlByaW5jaXBhbC5JSWRlbnRpdHkICAECAAAACgAAAAAGAwAAABdodWFuZ3hpbmdqaWVna2RAMTYzLmNvbQkEAAAAAAoPBAAAAAAAAAAICw=='
}

oriurl = "http://www.mangabz.com"
baseurl = "http://www.mangabz.com/search?title="  # åé¢æ¥çš„æ˜¯æœç´¢çš„å‚æ•°
imageappend = "chapterimage.ashx?"
imgbase = "http://image.mangabz.com/"

page = 0
cid = 0
mid = 0
key = 0

params = {
    'cid': cid,
    'page': page,
    'key': 0,
    '_cid': cid,
    '_mid': 11075,
    '_dt': '2021-02-08 18:20:06',
    '_sign': '207197d11fd7853fb4e7acdfe5e43e85'
}


def main():
    print("==============================")
    print("=æ¬¢è¿ä½¿ç”¨æ¼«ç”»èŠ±é‡Œèƒ¡å“¨ä¸‹è½½è„šæœ¬=")
    print("=å¼ºçƒˆå»ºè®®åœ¨ç½‘ç»œè‰¯å¥½æƒ…å†µä¸‹ä½¿ç”¨=")
    print("==============================", end="\n\n\n")
    anime = input("è¯·è¾“å…¥ä½ çš„è¦ä¸‹è½½çš„æ¼«ç”»å:")
    print("æ­£åœ¨æœç´¢ï¼Œè¯·ç­‰å¾…ï¼    ğŸ™‚", end="\n\n")
    try:
        req = requests.get(baseurl + anime, headers=headers)
    except:
        print("å®³ï¼ŒğŸ¤¦ï¼Œä½ æ˜¯ä¸æ˜¯æ²¡è”ç½‘ï¼â€")
    try:
        urlUpdate = showname(pq(req.text))
    except:
        print("å¯èƒ½ç½‘ç»œä¸è¡Œ")
    print(urlUpdate)
    checking = input("è¯·ç¡®è®¤æ˜¯å“ªä¸€ä¸ªï¼šï¼ˆå¦‚æœæ²¡æœ‰ï¼Œ è¾“å…¥-----æ— ï¼‰")
    url = oriurl + geturlUpdate(checking, urlUpdate)
    check404(url)
    nameTitle = Source(url)
    print("æƒ³ä¸‹è½½ä»å“ªåˆ°å“ªâ“")
    frompoint = int(input("ä»:"))
    topoint = int(input("åˆ°:"))
    print("ä½ æƒ³å­˜åˆ°å“ªé‡Œå‘¢ï¼Ÿ")
    location = input("è¯·è¾“å…¥æ­£ç¡®çš„è·¯å¾„ï¼š")
    os.chdir(location)
    os.mkdir(anime)
    os.chdir(location + "\\" + anime)
    print("ok!", "è¿™å°±ä¸ºæ‚¨æ•ˆåŠ³ï¼")
    pageurl = getPageUrl(url, begin=frompoint, end=topoint)
    for i, url in enumerate(pageurl):
        os.mkdir(nameTitle[i])
        os.chdir(nameTitle[i] + "\\")
        print("æ­£åœ¨ä¸‹è½½ç¬¬" + str(i + 1) + "é›†")
        getimageurl(url)
        os.chdir('..\\')


def getimageurl(url: str, ):
    req = requests.get(url, headers=headers)
    doc = pq(req.text)
    maxpage = int(doc('#lbcurrentpage').parent().text().split('-')[-1])  # è·å¾—æœ€å¤§é¡µæ•°
    print('Max Page', maxpage)
    cid = int(url.split('/')[-2][1:])  # æ­£ç¡®
    pattern = re.compile(r'dm5imagefun(.*?)_', re.S)
    params['_cid'] = cid
    params['cid'] = cid
    params['page'] = 1
    imgreq = url + imageappend + urlencode(params)
    text: str = str(requests.get(imgreq, headers=headers).text)
    result = str(re.findall(pattern, text)[0]).split('|')
    Index1 = 0
    Index2 = 0
    key: str
    imgoriurl: str
    index: str = ""
    if result[-3].isdigit():
        Index1 = str(result[-2])
        Index2 = str(result[-3])
        key = str(result[-4])
    else:
        Index1 = "1"
        Index2 = str(result[-2])
        key = str(result[-3])
    param = {
        'cid': cid,
        'key': key,
        'uk': ""
    }
    for i in range(maxpage):
        params['page'] = i + 1
        print("Page" + str(i + 1) + 'æ­£åœ¨ä¸‹è½½', end='----------')
        imgreq = url + imageappend + urlencode(params)
        req = requests.get(imgreq, headers)
        doc = str(req.text)
        spldoc = doc.split('|')
        if i + 1 != maxpage:
            index = spldoc[-14]
        else:
            index = spldoc[-15]
        imgurl = imgbase + Index1 + '/' + Index2 + '/' + str(cid) + '/' + index + '.jpg?' + urlencode(param)
        img = requests.get(imgurl, headers)
        with open(str(i + 1) + '.jpg', 'wb') as f:
            f.write(img.content)
        print('ä¸‹è½½å®Œæˆï¼')


def getPageUrl(url, begin, end):
    req = requests.get(url, headers=headers)
    doc = pq(req.text)
    aTag = doc('div .detail-list-form-con').children('a')
    href = []
    for elem in aTag.items():
        href.append(oriurl + elem.attr.href)
    href = href[::-1][begin - 1:end]
    return href


def Source(url):
    req = requests.get(url, headers=headers)
    doc = pq(req.text)
    tagA = doc('div .detail-list-form-con').children('a')
    nameTitle = []
    for a in tagA.items():
        nameTitle.append(a.text().split()[0] + a.text().split()[1])
    nameTitle = nameTitle[::-1]
    nameresult = Series(nameTitle, index=range(len(nameTitle)))
    for i, name in nameresult.items():
        print(int(i) + 1, name)
    print('æ€»:', len(nameTitle))
    return nameTitle


def geturlUpdate(checking: str, urlUpdate: DataFrame) -> str:
    animeIndex = -1
    if checking == "æ— ":
        print("è°¢è°¢ä½¿ç”¨ï¼")
        sys.exit()
    if checking.isdigit():
        animeIndex = urlUpdate.index.to_list()[int(checking)]
    elif checking in urlUpdate['æ¼«ç”»å'].values.tolist():
        animeIndex = urlUpdate[urlUpdate['æ¼«ç”»å'] == checking].index.to_list()[0]
    elif checking in urlUpdate['ç½‘ç«™åç¼€'].values.tolist():
        animeIndex = urlUpdate[urlUpdate['ç½‘ç«™åç¼€'] == checking].index.to_list()[0]
    else:
        print("ğŸ˜‚ğŸ˜‚")
        print("è¯·ä¸è¦è¯•å›¾å˜²è®½æˆ‘è¿™ä¸€ä¸ªç®€å•çš„è„šæœ¬ï¼")
        print("æˆ‘åªæƒ³å¥½å¥½çš„çˆ¬èµ„æºï¼")
        print("---------------")
        print("è¿˜æ˜¯ä»˜é”™äº†äººï¼ï¼ï¼")
        sys.exit()
    return str(urlUpdate['ç½‘ç«™åç¼€'][animeIndex])
    pass


def check404(url):
    print("æ­£åœ¨æ£€æµ‹æ˜¯å¦è¿˜æœ‰èµ„æºï¼")
    req = requests.get(url, headers=headers)
    doc = pq(req.text)
    if doc('.img-404'):
        print("è¿™ä¸ªæ¼«ç”»è¢«ç¦äº†ï¼Œæ²¡æœ‰èµ„æºï¼")
        sys.exit()
    else:
        print("OK")
        print("è¿™å°±ç€æ‰‹ä¸‹è½½ã€‚")
    pass


def showname(text: PyQuery):
    text = text('h2 a')
    hrefUrl = []
    animeName = []
    for a in text.items():
        hrefUrl.append(a.attr.href)
        animeName.append(a.attr.title)
    anime_href = DataFrame({
        "æ¼«ç”»å": animeName,
        "ç½‘ç«™åç¼€": hrefUrl,
    })

    return anime_href


if __name__ == "__main__":
    main()
    print("è°¢è°¢ä½¿ç”¨!ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€")
    pass
